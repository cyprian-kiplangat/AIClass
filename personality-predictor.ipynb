{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PESONALITY PREDICTOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of content :**\n",
    "\n",
    "1. IMPOPRTING LIBRARIES AND LOADING DATA\n",
    "\n",
    "2. DATA CLEANING & PREPROCESSING\n",
    "\n",
    "3. EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "4. FEATURE ENGINEERING\n",
    "\n",
    "5. MODEL\n",
    "\n",
    "6. GIVE INPUTS \n",
    "\n",
    "7. FIND THE OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPOPRTING LIBRARIES AND LOADING DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-04T09:08:21.977502Z",
     "iopub.status.busy": "2025-06-04T09:08:21.977232Z",
     "iopub.status.idle": "2025-06-04T09:08:26.711769Z",
     "shell.execute_reply": "2025-06-04T09:08:26.710626Z",
     "shell.execute_reply.started": "2025-06-04T09:08:21.977470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T09:09:12.086118Z",
     "iopub.status.busy": "2025-06-04T09:09:12.085724Z",
     "iopub.status.idle": "2025-06-04T09:09:12.102580Z",
     "shell.execute_reply": "2025-06-04T09:09:12.101463Z",
     "shell.execute_reply.started": "2025-06-04T09:09:12.086062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/mutaician/.cache/kagglehub/datasets/rakeshkapilavai/extrovert-vs-introvert-behavior-data/versions/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time_spent_Alone</th>\n",
       "      <th>Stage_fear</th>\n",
       "      <th>Social_event_attendance</th>\n",
       "      <th>Going_outside</th>\n",
       "      <th>Drained_after_socializing</th>\n",
       "      <th>Friends_circle_size</th>\n",
       "      <th>Post_frequency</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Introvert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Extrovert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
       "0               4.0         No                      4.0            6.0   \n",
       "1               9.0        Yes                      0.0            0.0   \n",
       "2               9.0        Yes                      1.0            2.0   \n",
       "3               0.0         No                      6.0            7.0   \n",
       "4               3.0         No                      9.0            4.0   \n",
       "\n",
       "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
       "0                        No                 13.0             5.0   Extrovert  \n",
       "1                       Yes                  0.0             3.0   Introvert  \n",
       "2                       Yes                  5.0             2.0   Introvert  \n",
       "3                        No                 14.0             8.0   Extrovert  \n",
       "4                        No                  8.0             5.0   Extrovert  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rakeshkapilavai/extrovert-vs-introvert-behavior-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "data = pd.read_csv(path + \"/personality_dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING AND PREPROCESSING\n",
    "\n",
    "**In this section, we systematically prepare our raw data for analysis by addressing quality issues and ensuring consistency. Each step is clearly explained and verified.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initial Data Exploration\n",
    "**Goal**: Understand the structure, size, and basic characteristics of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: INITIAL DATA EXPLORATION ===\n",
      "Dataset Shape: (2900, 8)\n",
      "Total Records: 2900\n",
      "Total Features: 8\n",
      "\n",
      "Column Names and Data Types:\n",
      "Time_spent_Alone             float64\n",
      "Stage_fear                    object\n",
      "Social_event_attendance      float64\n",
      "Going_outside                float64\n",
      "Drained_after_socializing     object\n",
      "Friends_circle_size          float64\n",
      "Post_frequency               float64\n",
      "Personality                   object\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n",
      "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "0               4.0         No                      4.0            6.0   \n",
      "1               9.0        Yes                      0.0            0.0   \n",
      "2               9.0        Yes                      1.0            2.0   \n",
      "3               0.0         No                      6.0            7.0   \n",
      "4               3.0         No                      9.0            4.0   \n",
      "\n",
      "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
      "0                        No                 13.0             5.0   Extrovert  \n",
      "1                       Yes                  0.0             3.0   Introvert  \n",
      "2                       Yes                  5.0             2.0   Introvert  \n",
      "3                        No                 14.0             8.0   Extrovert  \n",
      "4                        No                  8.0             5.0   Extrovert  \n",
      "\n",
      "Basic Statistics:\n",
      "       Time_spent_Alone  Social_event_attendance  Going_outside  \\\n",
      "count       2837.000000              2838.000000    2834.000000   \n",
      "mean           4.505816                 3.963354       3.000000   \n",
      "std            3.479192                 2.903827       2.247327   \n",
      "min            0.000000                 0.000000       0.000000   \n",
      "25%            2.000000                 2.000000       1.000000   \n",
      "50%            4.000000                 3.000000       3.000000   \n",
      "75%            8.000000                 6.000000       5.000000   \n",
      "max           11.000000                10.000000       7.000000   \n",
      "\n",
      "       Friends_circle_size  Post_frequency  \n",
      "count          2823.000000     2835.000000  \n",
      "mean              6.268863        3.564727  \n",
      "std               4.289693        2.926582  \n",
      "min               0.000000        0.000000  \n",
      "25%               3.000000        1.000000  \n",
      "50%               5.000000        3.000000  \n",
      "75%              10.000000        6.000000  \n",
      "max              15.000000       10.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 1: INITIAL DATA EXPLORATION ===\")\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(f\"Total Records: {data.shape[0]}\")\n",
    "print(f\"Total Features: {data.shape[1]}\")\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Missing Values Analysis\n",
    "**Goal**: Identify and handle any missing data to ensure data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 2: MISSING VALUES ANALYSIS ===\n",
      "Missing values per column:\n",
      "Time_spent_Alone             63\n",
      "Stage_fear                   73\n",
      "Social_event_attendance      62\n",
      "Going_outside                66\n",
      "Drained_after_socializing    52\n",
      "Friends_circle_size          77\n",
      "Post_frequency               65\n",
      "Personality                   0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 458\n",
      "Percentage of missing data: 15.79%\n",
      "\n",
      "Columns with missing values:\n",
      "  Time_spent_Alone: 63 missing (2.17%)\n",
      "  Stage_fear: 73 missing (2.52%)\n",
      "  Social_event_attendance: 62 missing (2.14%)\n",
      "  Going_outside: 66 missing (2.28%)\n",
      "  Drained_after_socializing: 52 missing (1.79%)\n",
      "  Friends_circle_size: 77 missing (2.66%)\n",
      "  Post_frequency: 65 missing (2.24%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 2: MISSING VALUES ANALYSIS ===\")\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "print(f\"Percentage of missing data: {(missing_values.sum() / len(data)) * 100:.2f}%\")\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    for col in missing_values[missing_values > 0].index:\n",
    "        pct = (missing_values[col] / len(data)) * 100\n",
    "        print(f\"  {col}: {missing_values[col]} missing ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n✅ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Duplicate Detection\n",
    "**Goal**: Identify and remove any duplicate records to maintain data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: DUPLICATE DETECTION ===\n",
      "Number of duplicate rows: 388\n",
      "Percentage of duplicates: 13.38%\n",
      "✅ Removed 388 duplicate rows\n",
      "Dataset shape after removing duplicates: (2512, 8)\n",
      "Final dataset size: 2512 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 3: DUPLICATE DETECTION ===\")\n",
    "initial_rows = len(data)\n",
    "duplicate_count = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Percentage of duplicates: {(duplicate_count / initial_rows) * 100:.2f}%\")\n",
    "    data = data.drop_duplicates()\n",
    "    print(f\"✅ Removed {duplicate_count} duplicate rows\")\n",
    "    print(f\"Dataset shape after removing duplicates: {data.shape}\")\n",
    "else:\n",
    "    print(\"✅ No duplicate rows found!\")\n",
    "\n",
    "print(f\"Final dataset size: {len(data)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Column Classification\n",
    "**Goal**: Classify columns into numeric and categorical for appropriate processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 4: COLUMN CLASSIFICATION ===\n",
      "Numeric columns (5): ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
      "Categorical columns (2): ['Stage_fear', 'Drained_after_socializing']\n",
      "Target column: Personality\n",
      "✅ All columns properly classified!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 4: COLUMN CLASSIFICATION ===\")\n",
    "\n",
    "# Define column types based on the dataset structure\n",
    "numeric_columns = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', \n",
    "                  'Friends_circle_size', 'Post_frequency']\n",
    "categorical_columns = ['Stage_fear', 'Drained_after_socializing']\n",
    "target_column = 'Personality'\n",
    "\n",
    "print(f\"Numeric columns ({len(numeric_columns)}): {numeric_columns}\")\n",
    "print(f\"Categorical columns ({len(categorical_columns)}): {categorical_columns}\")\n",
    "print(f\"Target column: {target_column}\")\n",
    "\n",
    "# Verify all columns are accounted for\n",
    "all_feature_columns = numeric_columns + categorical_columns + [target_column]\n",
    "missing_cols = [col for col in data.columns if col not in all_feature_columns]\n",
    "if missing_cols:\n",
    "    print(f\"⚠️  Unclassified columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"✅ All columns properly classified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Quality Checks\n",
    "**Goal**: Verify that our data values make logical sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 5: DATA QUALITY CHECKS ===\n",
      "Checking for negative values in numeric columns:\n",
      "  ✅ Time_spent_Alone: No negative values\n",
      "  ✅ Social_event_attendance: No negative values\n",
      "  ✅ Going_outside: No negative values\n",
      "  ✅ Friends_circle_size: No negative values\n",
      "  ✅ Post_frequency: No negative values\n",
      "\n",
      "Value ranges for numeric columns:\n",
      "  Time_spent_Alone: Range [0.0 - 11.0], Mean: 4.2\n",
      "  Social_event_attendance: Range [0.0 - 10.0], Mean: 4.2\n",
      "  Going_outside: Range [0.0 - 7.0], Mean: 3.2\n",
      "  Friends_circle_size: Range [0.0 - 15.0], Mean: 6.6\n",
      "  Post_frequency: Range [0.0 - 10.0], Mean: 3.8\n",
      "\n",
      "Categorical column values:\n",
      "  Stage_fear: ['No' 'Yes' nan]\n",
      "  Drained_after_socializing: ['No' 'Yes' nan]\n",
      "\n",
      "Target variable (Personality) distribution:\n",
      "  Extrovert: 1417 (56.4%)\n",
      "  Introvert: 1095 (43.6%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 5: DATA QUALITY CHECKS ===\")\n",
    "\n",
    "# Check for negative values in columns where they shouldn't exist\n",
    "print(\"Checking for negative values in numeric columns:\")\n",
    "for col in numeric_columns:\n",
    "    negative_count = (data[col] < 0).sum()\n",
    "    if negative_count > 0:\n",
    "        print(f\"  ⚠️  {col}: {negative_count} negative values\")\n",
    "    else:\n",
    "        print(f\"  ✅ {col}: No negative values\")\n",
    "\n",
    "# Check value ranges for reasonableness\n",
    "print(\"\\nValue ranges for numeric columns:\")\n",
    "for col in numeric_columns:\n",
    "    min_val = data[col].min()\n",
    "    max_val = data[col].max()\n",
    "    mean_val = data[col].mean()\n",
    "    print(f\"  {col}: Range [{min_val:.1f} - {max_val:.1f}], Mean: {mean_val:.1f}\")\n",
    "\n",
    "# Check categorical column values\n",
    "print(\"\\nCategorical column values:\")\n",
    "for col in categorical_columns:\n",
    "    unique_vals = data[col].unique()\n",
    "    print(f\"  {col}: {unique_vals}\")\n",
    "\n",
    "# Check target variable distribution\n",
    "print(f\"\\nTarget variable ({target_column}) distribution:\")\n",
    "target_counts = data[target_column].value_counts()\n",
    "for personality, count in target_counts.items():\n",
    "    percentage = (count / len(data)) * 100\n",
    "    print(f\"  {personality}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Handle Missing Values \n",
    "**Goal**: Apply appropriate imputation strategies if missing values are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 6: HANDLE MISSING VALUES ===\n",
      "Applying imputation strategies...\n",
      "✅ Numeric columns: Missing values filled with median\n",
      "✅ Categorical columns: Missing values filled with mode\n",
      "\n",
      "Remaining missing values: 0\n",
      "Dataset shape after handling missing values: (2512, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 6: HANDLE MISSING VALUES ===\")\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "data_clean = data.copy()\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Applying imputation strategies...\")\n",
    "    \n",
    "    # Impute numeric columns with median (robust to outliers)\n",
    "    if any(data_clean[numeric_columns].isnull().sum() > 0):\n",
    "        numeric_imputer = SimpleImputer(strategy='median')\n",
    "        data_clean[numeric_columns] = numeric_imputer.fit_transform(data_clean[numeric_columns])\n",
    "        print(\"✅ Numeric columns: Missing values filled with median\")\n",
    "    \n",
    "    # Impute categorical columns with mode (most frequent value)\n",
    "    if any(data_clean[categorical_columns].isnull().sum() > 0):\n",
    "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        data_clean[categorical_columns] = categorical_imputer.fit_transform(data_clean[categorical_columns])\n",
    "        print(\"✅ Categorical columns: Missing values filled with mode\")\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    remaining_missing = data_clean.isnull().sum().sum()\n",
    "    print(f\"\\nRemaining missing values: {remaining_missing}\")\n",
    "else:\n",
    "    print(\"✅ No missing values to handle!\")\n",
    "\n",
    "print(f\"Dataset shape after handling missing values: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Encode Categorical Variables\n",
    "**Goal**: Convert categorical variables to numeric format for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 7: ENCODE CATEGORICAL VARIABLES ===\n",
      "Target variable encoded: {'Extrovert': np.int64(0), 'Introvert': np.int64(1)}\n",
      "\n",
      "Applying one-hot encoding to categorical features...\n",
      "Original shape: (2512, 8)\n",
      "After encoding: (2512, 8)\n",
      "New columns added: 0\n",
      "New encoded columns: ['Stage_fear_Yes', 'Drained_after_socializing_Yes']\n",
      "✅ Categorical encoding completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 7: ENCODE CATEGORICAL VARIABLES ===\")\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "data_clean[target_column] = label_encoder.fit_transform(data_clean[target_column])\n",
    "print(f\"Target variable encoded: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "\n",
    "# One-hot encode other categorical variables\n",
    "print(\"\\nApplying one-hot encoding to categorical features...\")\n",
    "data_encoded = pd.get_dummies(data_clean, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "print(f\"Original shape: {data_clean.shape}\")\n",
    "print(f\"After encoding: {data_encoded.shape}\")\n",
    "print(f\"New columns added: {data_encoded.shape[1] - data_clean.shape[1]}\")\n",
    "\n",
    "# Display new column names\n",
    "new_columns = [col for col in data_encoded.columns if col not in data_clean.columns]\n",
    "if new_columns:\n",
    "    print(f\"New encoded columns: {new_columns}\")\n",
    "\n",
    "print(\"✅ Categorical encoding completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train-Test Split\n",
    "**Goal**: Split the data into training and testing sets for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 8: TRAIN-TEST SPLIT ===\n",
      "Features shape: (2512, 7)\n",
      "Target shape: (2512,)\n",
      "Feature columns: ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency', 'Stage_fear_Yes', 'Drained_after_socializing_Yes']\n",
      "\n",
      "Training set: 2009 samples\n",
      "Test set: 503 samples\n",
      "Split ratio: 80% train, 20% test\n",
      "\n",
      "Class distribution in training set:\n",
      "  Extrovert: 56.4%\n",
      "  Introvert: 43.6%\n",
      "\n",
      "Class distribution in test set:\n",
      "  Extrovert: 56.5%\n",
      "  Introvert: 43.5%\n",
      "\n",
      "✅ Data preprocessing completed successfully!\n",
      "\n",
      "==================================================\n",
      "PREPROCESSING SUMMARY:\n",
      "• Original dataset: (2512, 8)\n",
      "• Final processed dataset: (2512, 8)\n",
      "• Training samples: 2009\n",
      "• Test samples: 503\n",
      "• Features: 7\n",
      "• Ready for Exploratory Data Analysis!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STEP 8: TRAIN-TEST SPLIT ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X = data_encoded.drop(columns=[target_column])\n",
    "y = data_encoded[target_column]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")\n",
    "\n",
    "# Perform stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Split ratio: {X_train.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.0f}% train, {X_test.shape[0]/(X_train.shape[0]+X_test.shape[0])*100:.0f}% test\")\n",
    "\n",
    "# Verify class balance is maintained\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "train_dist = y_train.value_counts(normalize=True)\n",
    "for class_val, percentage in train_dist.items():\n",
    "    class_name = label_encoder.inverse_transform([class_val])[0]\n",
    "    print(f\"  {class_name}: {percentage:.1%}\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "test_dist = y_test.value_counts(normalize=True)\n",
    "for class_val, percentage in test_dist.items():\n",
    "    class_name = label_encoder.inverse_transform([class_val])[0]\n",
    "    print(f\"  {class_name}: {percentage:.1%}\")\n",
    "\n",
    "print(\"\\n✅ Data preprocessing completed successfully!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREPROCESSING SUMMARY:\")\n",
    "print(f\"• Original dataset: {data.shape}\")\n",
    "print(f\"• Final processed dataset: {data_encoded.shape}\")\n",
    "print(f\"• Training samples: {X_train.shape[0]}\")\n",
    "print(f\"• Test samples: {X_test.shape[0]}\")\n",
    "print(f\"• Features: {X_train.shape[1]}\")\n",
    "print(\"• Ready for Exploratory Data Analysis!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7474089,
     "sourceId": 11891195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AIClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
